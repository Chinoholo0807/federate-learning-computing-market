{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\cs\\Anaconda\\envs\\deeplearning_common\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'D:\\cs\\Anaconda\\envs\\deeplearning_common\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "# transformer =transforms.Compose(\n",
    "#     [transforms.ToTensor(),\n",
    "#      transforms.Normalize(mean = (0.5, 0.5, 0.5),std = (0.5, 0.5, 0.5))])\n",
    "trainset = torchvision.datasets.MNIST('./data',train=True,download=True,transform=transforms.ToTensor())\n",
    "testset = torchvision.datasets.MNIST('./data',train=False,download=True,transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(trainset,batch_size=batch_size,shuffle=True,num_workers=2)\n",
    "test_loader = DataLoader(testset,batch_size=batch_size,shuffle=True,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.datasets.mnist.MNIST'> \n",
      " <class 'torchvision.datasets.mnist.MNIST'> \n",
      " <class 'torch.utils.data.dataloader.DataLoader'> \n",
      " <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "<class 'torch.utils.data.dataloader._MultiProcessingDataLoaderIter'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "torch.Size([10, 1, 28, 28]) torch.Size([10])\n",
      "train_x(60000, 784),train_y(60000,),test_x(10000, 784),test_y(60000,)\n",
      "before  33.318421449829934\n",
      "after  0.13066062\n"
     ]
    }
   ],
   "source": [
    "print(type(trainset),'\\n',type(testset),'\\n',type(train_loader),'\\n',type(test_loader))\n",
    "train_iter = iter(train_loader)\n",
    "print(type(train_iter))\n",
    "train_x,train_y = train_iter.next()\n",
    "print(type(train_x),type(train_y))\n",
    "print(train_x.shape,train_y.shape)\n",
    "train_data = trainset.data.numpy()\n",
    "train_label = trainset.targets.numpy()\n",
    "test_data = testset.data.numpy()\n",
    "test_label = trainset.targets.numpy()\n",
    "train_data = train_data.reshape(train_data.shape[0],-1)\n",
    "test_data = test_data.reshape(test_data.shape[0],-1)\n",
    "print(f'train_x{train_data.shape},train_y{train_label.shape},test_x{test_data.shape},test_y{test_label.shape}')\n",
    "print('before ',train_data.mean())\n",
    "train_data = train_data.astype(np.float32)\n",
    "train_data = np.multiply(train_data, 1.0 / 255.0)\n",
    "print('after ',train_data.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "a = np.array([-0.14579831,  0.0507718 , -0.0928348 , -0.15952584,  0.03019532])\n",
    "b = np.array([-0.14579831,  0.0507718 , -0.0928348 , -0.15952584,  0.03019532])\n",
    "import torch.nn.functional as F\n",
    "dist = np.linalg.norm(a-b)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cacheout import Cache\n",
    "cache = Cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a None\n"
     ]
    }
   ],
   "source": [
    "cache.set(1,'a')\n",
    "assert cache.get(1) == 'a'\n",
    "assert cache.get(2) is None\n",
    "print(cache.get(1),cache.get(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "b'\\x80\\x04\\x95!2\\x00\\x00\\x00\\x00\\x00' b'\\x80\\x04\\x95!2\\x00\\x00\\x00\\x00\\x00'\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import pickle\n",
    "import hashlib\n",
    "net2 =copy.deepcopy(net)\n",
    "def state_equal(sd1,sd2):\n",
    "    for key in sd1:\n",
    "            # print(key)\n",
    "            if not sd1[key].equal(sd2[key]):\n",
    "                return False\n",
    "    return True\n",
    "def net_equal(net1,net2):\n",
    "    sd1 = net1.state_dict()\n",
    "    sd2 = net2.state_dict()\n",
    "    return state_equal(sd1,sd2)\n",
    "print(net_equal(net,net2))\n",
    "bytes_model2 = pickle.dumps(net2.state_dict())\n",
    "bytes_model1 = pickle.dumps(net.state_dict())\n",
    "print(bytes_model1[:10],bytes_model2[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2322760581008\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import time\n",
    "bytes_model1=pickle.dumps(net.state_dict)\n",
    "# bytes_model2=pickle.dumps(net.state_dict)\n",
    "# print(bytes_model1[:100],bytes_model2[:100])\n",
    "# print(bytes_model1 == bytes_model2)\n",
    "def get_hash(bytes_model):\n",
    "    hash = hashlib.md5()\n",
    "    print(id(hash))\n",
    "    hash.update(bytes_model)\n",
    "    # hash.update(b'haiwu')\n",
    "    hash.update(bytes(str(time.time()),encoding=\"utf-8\"))\n",
    "    hex = hash.hexdigest()\n",
    "    return hex\n",
    "hex1 = get_hash(bytes_model1)\n",
    "print(type(hex1))\n",
    "cache.add(hex1,bytes_model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2322760311440\n",
      "21c4b39662c4f9e792b906370745059f 21c4b39662c4f9e792b906370745059f\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "bytes_model1 = cache.get(hex1)\n",
    "hex2 = get_hash(bytes_model1)\n",
    "print(hex2,hex1)\n",
    "print(len(cache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04742587317756678 0\n",
      "0.11920292202211755 1\n",
      "0.2689414213699951 2\n",
      "0.5 3\n",
      "0.7310585786300049 4\n",
      "0.8807970779778823 5\n",
      "1.76 1.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "n_p = 5\n",
    "n_v = 3\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))\n",
    "for i in range(6):\n",
    "    print(sigmoid(i-n_v),i)\n",
    "print(0.88/0.5,5/3)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "932b7a23f5efe7e675e22d8246c22a7feb7a1efaaa56db3f91df2e4d4fb15687"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('deeplearning_common')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
